{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b747e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import autokeras as ak\n",
    "import tensorflow as tf\n",
    "from numpy import asarray\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a29d27a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create images array by selecting 5000 images from each domain and create csv with instances labels, label per domain\n",
    "directory = [\"knapsackImages\",\"imagesselectedset\",\"gcImages\"]\n",
    "images = []\n",
    "instancesTotal = 0\n",
    "total = 0\n",
    "Label = 0\n",
    "df = pd.DataFrame(columns = ['instance', 'Label'])\n",
    "for i in directory:\n",
    "    for filename in os.listdir(i):\n",
    "        if instancesTotal < 5000:\n",
    "            instance = os.path.join(i, filename)\n",
    "            instancesTotal+=1\n",
    "            total+=1\n",
    "            df.loc[total] = [filename,Label]\n",
    "            if instance.endswith('.jpg'):\n",
    "                im = Image.open(instance).convert('L')\n",
    "                pix = im.load()\n",
    "                width, height = im.size\n",
    "                pixel_values = list(im.getdata())\n",
    "                data = asarray(im)\n",
    "                images.append(data)\n",
    "            \n",
    "            \n",
    "    instancesTotal = 0\n",
    "    Label+=1\n",
    "    \n",
    "images2 = np.asarray(images)\n",
    "images2 = images2.reshape((images2.shape[0], 32, 32))\n",
    "df.to_csv('alldomains.csv', encoding='utf-8') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f75554",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the labels\n",
    "npdf = pd.read_csv('alldomains.csv')\n",
    "print(npdf['Label'].value_counts())\n",
    "source = npdf.Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cbc068",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = images2.astype('float32')\n",
    "X /= 255\n",
    "y = source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3276843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_callbacks = [\n",
    "    tf.keras.callbacks.CSVLogger(\"TSPimagesTraining.csv\", separator=\",\", append=False),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs',profile_batch = 100000000),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b2780c",
   "metadata": {},
   "source": [
    "# Autkeras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572ad472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score\n",
    "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "all_tests = []\n",
    "all_predictions = []\n",
    "for train_index, test_index in cv.split(X, y):\n",
    "  #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    clf = ak.ImageClassifier(overwrite= False, max_trials=1)\n",
    "  # Supervised training of the model\n",
    "    print(\"Start training\")\n",
    "    clf.fit(X_train, y_train, epochs=100, callbacks=tf_callbacks)\n",
    "\n",
    "    print(\"Predictions on unseen data\")\n",
    "    predicted_y = clf.predict(X_test)\n",
    "    all_tests.append(y_test)\n",
    "    all_predictions.append(predicted_y)\n",
    "    report = classification_report(y_test.astype(str), predicted_y.astype(str))\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8614b718",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for sublist in all_predictions:\n",
    "    for item in sublist:\n",
    "        predictions.append(int(item))\n",
    "\n",
    "tests = []\n",
    "for sublist in all_tests:\n",
    "    for item in sublist:\n",
    "        tests.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62638b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5000\n",
      "           1       1.00      0.91      0.95      5000\n",
      "           2       0.91      1.00      0.95      5000\n",
      "\n",
      "    accuracy                           0.97     15000\n",
      "   macro avg       0.97      0.97      0.97     15000\n",
      "weighted avg       0.97      0.97      0.97     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(tests,predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "566113df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9681333333333333\n",
      "Precision: 0.968\n",
      "Recall: 0.971\n",
      "F1-score: 0.968\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(predictions,tests))\n",
    "precision = precision_score(predictions,tests,average=\"macro\")\n",
    "recall = recall_score(predictions,tests,average=\"macro\")\n",
    "f1_score = f1_score(predictions,tests,average=\"macro\")\n",
    "print('Precision: %.3f' % precision)\n",
    "print('Recall: %.3f' % recall)\n",
    "print('F1-score: %.3f' % f1_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
